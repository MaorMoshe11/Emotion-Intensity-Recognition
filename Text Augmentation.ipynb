{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17b85Z-ribm_oeoMmgZ3P6cv5GmXxZrpV","timestamp":1756215379710}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Handling Imbalanced Classes with Augmentation\n","\n","In this notebook, we address the issue of **class imbalance**, where some emotion classes (minority classes) contain significantly fewer sentences than others.\n","\n","To balance the dataset:\n","- We first identify the **minority classes** by comparing the number of sentences across all classes.\n","- For each minority class, we calculate how many additional sentences are needed to match the size of the largest class.\n","- We then generate **augmented sentences** for these underrepresented classes using a controlled augmentation method, ensuring that the new data is meaningful and label-consistent.\n","\n","This results in a class-balanced dataset, which is better suited for training robust and fair models.\n"],"metadata":{"id":"AH83bdIAutGc"}},{"cell_type":"markdown","source":["Assume the data set is a dictionary with keys as classes and values lists of lists of words.\n","\n","1: [[w11,w12..],[w21,w22]...]\n","2: ..."],"metadata":{"id":"sAUrZasPoEM2"}},{"cell_type":"code","source":["!pip install --upgrade  gensim\n","!pip install nltk\n","!pip install wordcloud\n","!pip install scipy==1.12.1\n","!pip install numpy==1.26.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"TTozuU1w9reN","executionInfo":{"status":"ok","timestamp":1754483122425,"user_tz":-180,"elapsed":77844,"user":{"displayName":"×××•×¨ ××©×”","userId":"08411296119985004086"}},"outputId":"7bcbbc51-7574-4e90-de07-912996336967"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Collecting numpy<2.0,>=1.18.5 (from gensim)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n","  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n","Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.16.0\n","    Uninstalling scipy-1.16.0:\n","      Successfully uninstalled scipy-1.16.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"57baae8dbac642f58523b241bb500267"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n","Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from wordcloud) (1.26.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.3.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (25.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n","\u001b[31mERROR: Ignored the following yanked versions: 1.11.0, 1.14.0rc1\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement scipy==1.12.1 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.9.0rc1, 1.9.0rc2, 1.9.0rc3, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0rc1, 1.10.0rc2, 1.10.0, 1.10.1, 1.11.0rc1, 1.11.0rc2, 1.11.1, 1.11.2, 1.11.3, 1.11.4, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.13.0rc1, 1.13.0, 1.13.1, 1.14.0rc2, 1.14.0, 1.14.1, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for scipy==1.12.1\u001b[0m\u001b[31m\n","\u001b[0mRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"XpsU0hJzKZAh","executionInfo":{"status":"error","timestamp":1754483122861,"user_tz":-180,"elapsed":427,"user":{"displayName":"×××•×¨ ××©×”","userId":"08411296119985004086"}},"outputId":"0b67c636-436d-48b0-863b-88c4564539c1"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3889107961.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# Import top-level functionality into top-level namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemoize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatstruct\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/collocations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# these two unused imports are referenced in collocations.doctest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m from nltk.metrics import (\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mBigramAssocMeasures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mContingencyMeasures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/metrics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magreement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnotationTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from nltk.metrics.association import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mBigramAssocMeasures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mContingencyMeasures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/metrics/association.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfisher_exact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    604\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    605\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 606\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistance_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_importlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'scipy.{name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"]}]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","from collections import Counter\n","from itertools import chain\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","from google.colab import drive\n","import pandas as pd\n","import random\n","drive.mount('/content/drive')\n","import os\n","from typing import Iterable, Callable, List, Dict, Any,Tuple\n","from collections import defaultdict\n","from nltk.corpus import stopwords\n","import random"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ruW2KcYoEsy","executionInfo":{"status":"ok","timestamp":1754295318274,"user_tz":-180,"elapsed":2291,"user":{"displayName":"David Oriel","userId":"17342691093612237970"}},"outputId":"13c308a1-1f08-4a7f-e877-8672e4977907"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["#Load W2V model"],"metadata":{"id":"1ylI9HjEVidu"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","# this will download (once) and load the Word2Vec Google News vectors\n","w2v_model = api.load(\"word2vec-google-news-300\")\n"],"metadata":{"id":"ZSd_NjgvNyaY","executionInfo":{"status":"ok","timestamp":1754295395561,"user_tz":-180,"elapsed":77283,"user":{"displayName":"David Oriel","userId":"17342691093612237970"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"55893319-04b5-45a6-e53e-ba12ff1f5767"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('kings', 0.7138045430183411), ('queen', 0.6510956883430481), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204220056533813), ('prince', 0.6159993410110474)]\n"]}]},{"cell_type":"markdown","source":["#Insert Paths\n","Please enter the full path to the csv with the full sentences  \n","and the full path to the csv with each emotion rating.\n"],"metadata":{"id":"a_61p18g92gy"}},{"cell_type":"code","source":["words_csv_path = \"/content/drive/MyDrive/segmented_transcriptions_small.csv\" #set the full path to the csv with the words here\n","rating_csv_path = \"/content/drive/MyDrive/segmented_transcriptions_full.csv\" #set the full path to emotion ratings here\n","emotion = \"arousal\""],"metadata":{"id":"jTV045zuBT6L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_words = pd.read_csv(words_csv_path,encoding=\"latin1\")\n","df_rating =  pd.read_csv(rating_csv_path)\n"],"metadata":{"id":"xAV09TcKvVfQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_rating.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T6YmmKEe9nAt","executionInfo":{"status":"ok","timestamp":1754295395586,"user_tz":-180,"elapsed":20,"user":{"displayName":"David Oriel","userId":"17342691093612237970"}},"outputId":"4dcc5d25-384b-4e8e-e16b-554b2c6e0713"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['sub', 'episode', 'segment', 'start', 'end', 'irritation', 'nostalgia',\n","       'pride', 'relief', 'sadness', 'satisfaction', 'surprise', 'sympathy',\n","       'triumph', 'arousal', 'valence', 'contempt', 'contentment',\n","       'embarrassment', 'empathic_pain', 'envy', 'gratitude', 'disgust',\n","       'disappointment', 'despair', 'admiration', 'amusement',\n","       'aesthetic_appreciation', 'anger', 'anxiety', 'awe', 'calmness',\n","       'confusion', 'excitement', 'fear', 'guilt', 'interest', 'joy',\n","       'pleasure', 'romance', 'craving', 'entrancement', 'hope', 'boredom',\n","       'adoration', 'jealousy', 'horror', 'sexual_desire', 'ep_inds'],\n","      dtype='object')"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["##Organizing Sentences for Augmentation\n","\n","The code outputs a nested dictionary structure, where each top-level key corresponds to a specific emotion. Each of these sub-dictionaries maps numerical ratings (e.g., 1 to 7) to a list of sentences that received that particular rating for that emotion. Each sentence in the list is represented as a list of words (i.e., a tokenized sentence).\n"],"metadata":{"id":"zg78zzI1m7Y_"}},{"cell_type":"code","source":["def merge_words_ratings(\n","    df_words: pd.DataFrame,\n","    df_rating: pd.DataFrame,\n","    emotions: Iterable[str],\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Return a DataFrame with one row per sentence (from df_words),\n","    carrying along each requested emotion score (from df_rating).\n","\n","    Resulting columns: [\"sub\",\"episode\",\"segment\",\"label\"] + list(emotions)\n","    \"\"\"\n","    id_cols = [\"sub\", \"episode\", \"segment\"]\n","    # only keep id_cols + emotion columns in df_rating\n","    keep_cols = id_cols + list(emotions)\n","    df_trim = df_rating[keep_cols]\n","    # merge so that every sentence in df_words shows up, with its emotion scores (or NaN)\n","    df_merged = (\n","        df_words\n","        .merge(df_trim, on=id_cols, how=\"left\", sort=False)\n","        # ensure label is present\n","        .loc[:, id_cols + [\"label\"] + list(emotions)]\n","    )\n","    return df_merged\n","\n","\n","def build_word_dicts(\n","    df_merged: pd.DataFrame,\n","    emotions: Iterable[str],\n","    tokenize: Callable[[str], List[str]] = lambda s: s.split(),\n",") -> Dict[str, Dict[Any, List[Tuple[List[str], int]]]]:\n","    \"\"\"\n","    Given a merged DataFrame (one row per sentence, with columns:\n","            [\"sub\",\"episode\",\"segment\",\"label\"] + list(emotions)\n","    and whose index is the original rowâ€index),\n","    build, for each emotion, a dict mapping class_value â†’ list of\n","    (tokenized_sentence, original_row_index).\n","\n","    Parameters\n","    ----------\n","    df_merged : pd.DataFrame\n","        Must contain:\n","          - \"label\" column (the sentence)\n","          - one column per emotion in `emotions`\n","        And its index should correspond to the original df_merged indices.\n","    emotions : iterable of str\n","        The names of the emotionâ€columns to build dicts for.\n","    tokenize : Callable[[str], List[str]]\n","        How to split a sentence into words.\n","\n","    Returns\n","    -------\n","    Dict[str, Dict[Any, List[Tuple[List[str], int]]]]\n","        A mapping from each emotion name to its word_dict. Each word_dict:\n","            class_value â†’ [ ( [w1, w2, â€¦], row_idx ), â€¦ ]\n","    \"\"\"\n","    # prepare one defaultdict(list) per emotion\n","    out: Dict[str, Dict[Any, List[Tuple[List[str], int]]]] = {\n","        emo: defaultdict(list) for emo in emotions\n","    }\n","\n","    # iterate once over every row (gets you row.Index as the DF index)\n","    for row in df_merged.itertuples(index=True):\n","        tokens = tokenize(row.label)  # split the sentence\n","        idx    = row.Index           # the original mergedâ€df index\n","\n","        for emo in emotions:\n","            cls = getattr(row, emo)  # fetch the value in that emotionâ€column\n","            out[emo][cls].append((tokens, idx))\n","\n","    # turn defaultdicts into normal dicts\n","    return {emo: dict(d) for emo, d in out.items()}\n","\n","merged = merge_words_ratings(df_words, df_rating, [\"arousal\",\"sadness\",\"fear\",\"joy\",\"disgust\",\"anger\",\"surprise\"])\n","\n","\n","# 2) then build perâ€emotion word dict\n","word_dicts = build_word_dicts(merged, [\"arousal\"])\n","\n","word_dict_arousal  = word_dicts[\"arousal\"]"],"metadata":{"id":"k-_tzGyMmH_b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## augmentation part\n","\n","The code below takes the word dictionary generated in the previous step and computes the LIC score for each word. It also determines, for each class, how many additional sentences are needed in order to balance the dataset â€” i.e., so that all classes contain the same number of sentences as the class with the highest count.\n","\n","Next, we iterate over each class and generate synthetic sentences to fill the gap. For each new sentence, we select the word with the highest LIC score that is not a stopword, and replace it with the most similar word based on a pretrained Word2Vec model.\n","\n","By the end of this process, the word dictionary is extended with newly augmented sentences, resulting in a class-balanced dataset ready for training."],"metadata":{"id":"rM6J6sW600kL"}},{"cell_type":"code","source":["stop_words = set(stopwords.words(\"english\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f-Gucn8fULxE","executionInfo":{"status":"ok","timestamp":1754295395782,"user_tz":-180,"elapsed":45,"user":{"displayName":"David Oriel","userId":"17342691093612237970"}},"outputId":"0d858924-9076-4ea5-cc71-ba4c0acf0e02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'don', 'now', 've', 'against', 'having', 'its', \"it'll\", 'haven', 'doesn', \"i'm\", 'can', \"we've\", 'yourselves', 'yourself', 'over', 'an', 'had', \"hasn't\", 'has', 'few', 'are', 'am', 'me', 'is', 'too', 'with', 'how', \"we're\", \"needn't\", 'down', \"wouldn't\", \"that'll\", \"you've\", 'does', 'both', 'about', 'be', 'they', 'than', 'there', 'each', 'because', 'but', 'did', 'this', 's', \"we'd\", 'who', 'most', 'him', 'nor', \"she'd\", 'hasn', \"it'd\", \"it's\", 'until', 'again', 'of', 'hers', 'that', 'off', \"you'll\", 'from', \"you're\", 'll', \"won't\", 'the', 'will', 'myself', 'i', 'm', 'such', 'do', 'to', \"shan't\", 'being', \"didn't\", \"i'd\", 'here', 'itself', 'needn', 'by', 'should', \"he'll\", 'shouldn', 'at', \"he'd\", 'out', 'herself', 'only', \"mightn't\", \"aren't\", 'a', 'doing', 'above', 'his', 'as', 'she', \"they'd\", 'mustn', 'my', 'once', 'all', 'weren', 'then', \"should've\", 'if', 'we', 'after', 'aren', 'ma', 'our', 'on', 'were', 'themselves', 'when', 'what', 'hadn', 'd', 'wouldn', 'below', 'where', 'you', 'their', \"couldn't\", \"she'll\", 'ours', \"shouldn't\", \"i'll\", 'same', 'o', 'those', 'them', 'up', 't', 'yours', \"i've\", 'wasn', 'any', \"haven't\", 'it', 'more', 'ourselves', 're', 'no', 'in', 'under', 'and', \"she's\", 'couldn', 'which', 'didn', \"isn't\", \"they're\", 'y', 'your', \"hadn't\", 'not', \"he's\", \"they'll\", 'while', 'or', 'himself', 'whom', 'so', 'very', 'won', 'for', 'have', \"you'd\", 'these', 'before', 'other', 'her', \"they've\", 'ain', 'through', \"doesn't\", 'why', 'own', \"weren't\", 'mightn', 'theirs', 'into', 'isn', \"we'll\", 'some', 'he', 'during', 'was', 'been', 'just', 'between', \"wasn't\", \"don't\", 'further', 'shan', \"mustn't\"}\n"]}]},{"cell_type":"code","source":["from typing import Callable, List, Dict\n","stop_words = set(stopwords.words(\"english\"))\n","\n","def compute_lic(word_dict: Dict) -> Dict:\n","    def class_word_counts(data):\n","        out = {}\n","        for cls, tuples in data.items():\n","            flattened = Counter(word for sub in tuples for word in sub[0])\n","            out.update({(cls, w): c for w, c in flattened.items()})\n","        return out\n","\n","    c_w_c = class_word_counts(word_dict)\n","    lic_df = (\n","        pd.Series(c_w_c, name=\"count\")\n","        .rename_axis(index=[\"class\", \"word\"])\n","        .reset_index()\n","    )\n","    lic_df[\"tf\"] = lic_df[\"count\"] / lic_df.groupby(\"class\")[\"count\"].transform(\"sum\")\n","\n","    pivot = lic_df.pivot_table(index=\"word\", columns=\"class\", values=\"tf\", fill_value=0.0)\n","    C = len(word_dict)\n","    df_w = (pivot > 0).sum(axis=1)\n","    idf = np.log(C / (1 + df_w)) + 1\n","\n","    lic_records = []\n","    for c in pivot.columns:\n","        tf_c = pivot[c]\n","        tf_not_c = pivot.drop(columns=c)\n","        mu_not_c = tf_not_c.mean(axis=1)\n","        sigma_not_c = tf_not_c.std(axis=1).replace(0, np.nan)\n","\n","        z = (tf_c - mu_not_c) / sigma_not_c\n","        licc = z * idf\n","        lic_records.append(licc.rename(c))\n","\n","    lic_df = pd.concat(lic_records, axis=1).stack(dropna=True)\n","    lic_series = lic_df.swaplevel(0, 1).sort_index()\n","    lic_series.index.names = [\"class\", \"word\"]\n","    return lic_series.to_dict()\n","\n","\n","def count_missing_samples(word_dict: Dict) -> Dict:\n","    \"\"\"\n","    counts how many samples are missing in that class untill it will be full\n","    \"\"\"\n","    max_size = max(len(samples) for samples in word_dict.values())\n","    return {\n","        cls: max_size - len(samples)\n","        for cls, samples in word_dict.items()\n","    }\n","\n","\n","def generate_samples_for_class(word_dict, key, missing, LIC, w2v_model):\n","    existing = word_dict.get(key, [])\n","    n_exist = len(existing)\n","    if n_exist == 0 or missing <= 0:\n","        return []\n","\n","    new_samples = []\n","    used_pairs = set()\n","    attempts = 0\n","    max_attempts = missing * 10\n","\n","    while len(new_samples) < missing and attempts < max_attempts:\n","        attempts += 1\n","        idx = random.randrange(n_exist)\n","        sentence, df_idx  = existing[idx]\n","\n","        lic_list = [(pos, w, LIC.get((key, w), 0.0)) for pos, w in enumerate(sentence) if w.lower() not in stop_words]\n","        lic_list.sort(key=lambda x: -x[2])\n","        for pos, word, score in lic_list:\n","            if score <= 0 or (idx, pos) in used_pairs:\n","                continue\n","            try:\n","                sims = w2v_model.most_similar(positive=[word], topn=10)\n","            except KeyError:\n","                continue\n","            replacement = next((w for w, _ in sims if w != word), None)\n","            if not replacement:\n","                continue\n","\n","            new_sent = sentence.copy()\n","            new_sent[pos] = replacement\n","            new_samples.append((new_sent,df_idx))\n","            used_pairs.add((idx, pos))\n","            break\n","\n","    word_dict[key].extend(new_samples)\n","    return new_samples\n","\n","\n","def generate_samples(word_dict, LIC, w2v_model):\n","    missing_counts = count_missing_samples(word_dict)\n","    for cls, missing in missing_counts.items():\n","        generate_samples_for_class(word_dict, cls, missing, LIC, w2v_model)\n","    return word_dict\n","\n","\n","def augment_all_emotions(word_dicts, emotions, w2v_model) -> Dict[str, dict]:\n","    augmented = {}\n","    for emo in emotions:\n","        word_dict = word_dicts[emo]\n","        lic = compute_lic(word_dict)\n","        balanced = generate_samples(word_dict, lic, w2v_model)\n","        augmented[emo] = balanced\n","    return augmented"],"metadata":{"id":"OTupr44dUO4v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emotions = [\"arousal\"]\n","augmented_word_dicts = augment_all_emotions(word_dicts, emotions, w2v_model)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Y206STPUPo4","executionInfo":{"status":"ok","timestamp":1754295626149,"user_tz":-180,"elapsed":7140,"user":{"displayName":"David Oriel","userId":"17342691093612237970"}},"outputId":"8726b7a2-fc5e-49da-8c67-3b4cdb3d76e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ”§ Augmenting for emotion: arousal\n","[(30, 'giant', 63.12511946764411), (31, 'giant', 63.12511946764411), (6, 'brother', 31.190837938241714), (25, 'hands', 20.120287008048873), (11, 'snail', 12.030269020600269), (32, 'snail', 12.030269020600269), (9, 'found', 9.90131691864011), (4, 'time', 7.400576964778002), (15, 'honestly', 0.0), (19, 'size', 0.0), (22, 'four', 0.0), (26, 'like', -1.0797454835905596)]\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-814560459.py:36: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n","  lic_df = pd.concat(lic_records, axis=1).stack(dropna=True)\n"]},{"output_type":"stream","name":"stdout","text":["[(30, 'giant', 63.12511946764411), (31, 'giant', 63.12511946764411), (6, 'brother', 31.190837938241714), (25, 'hands', 20.120287008048873), (11, 'snail', 12.030269020600269), (32, 'snail', 12.030269020600269), (9, 'found', 9.90131691864011), (3, 'one', 9.090287546464811), (4, 'time', 7.400576964778002), (15, 'honestly', 0.0), (19, 'size', 0.0), (22, 'four', 0.0), (26, 'like', -1.0797454835905596)]\n","[(30, 'giant', 63.12511946764411), (6, 'brother', 31.190837938241714), (25, 'hands', 20.120287008048873), (11, 'snail', 12.030269020600269), (32, 'snail', 12.030269020600269), (9, 'found', 9.90131691864011), (3, 'one', 9.090287546464811), (4, 'time', 7.400576964778002), (15, 'honestly', 0.0), (19, 'size', 0.0), (22, 'four', 0.0), (31, 'behemoth', 0.0), (26, 'like', -1.0797454835905596)]\n","[(30, 'giant', 63.12511946764411), (31, 'giant', 63.12511946764411), (25, 'hands', 20.120287008048873), (11, 'snail', 12.030269020600269), (32, 'snail', 12.030269020600269), (9, 'found', 9.90131691864011), (3, 'one', 9.090287546464811), (4, 'time', 7.400576964778002), (6, 'younger_brother', 0.0), (15, 'honestly', 0.0), (19, 'size', 0.0), (22, 'four', 0.0), (26, 'like', -1.0797454835905596)]\n","[(30, 'giant', 63.12511946764411), (31, 'giant', 63.12511946764411), (6, 'brother', 31.190837938241714), (25, 'hands', 20.120287008048873), (11, 'snail', 12.030269020600269), (32, 'snail', 12.030269020600269), (9, 'found', 9.90131691864011), (3, 'one', 9.090287546464811), (4, 'day', 0.5627800384164827), (15, 'honestly', 0.0), (19, 'size', 0.0), (22, 'four', 0.0), (26, 'like', -1.0797454835905596)]\n","[(30, 'giant', 63.12511946764411), (31, 'giant', 63.12511946764411), (6, 'brother', 31.190837938241714), (25, 'hands', 20.120287008048873), (11, 'snail', 12.030269020600269), (9, 'found', 9.90131691864011), (3, 'one', 9.090287546464811), (4, 'time', 7.400576964778002), (32, 'snails', 1.6220587443505983), (15, 'honestly', 0.0), (19, 'size', 0.0), (22, 'four', 0.0), (26, 'like', -1.0797454835905596)]\n","[(31, 'giant', 63.12511946764411), (6, 'brother', 31.190837938241714), (25, 'hands', 20.120287008048873), (11, 'snail', 12.030269020600269), (32, 'snail', 12.030269020600269), (9, 'found', 9.90131691864011), (3, 'one', 9.090287546464811), (4, 'time', 7.400576964778002), (15, 'honestly', 0.0), (19, 'size', 0.0), (22, 'four', 0.0), (30, 'behemoth', 0.0), (26, 'like', -1.0797454835905596)]\n"]}]},{"cell_type":"markdown","source":["##Restore emotion score for each sentence\n","\n","### Restoring Emotion Scores\n","\n","In the final step, we restore the **emotion scores** for each sentence â€” including both original and augmented samples.\n","\n","- **Original sentences** keep their original emotion scores as they appeared in the dataset.\n","- **Augmented sentences** inherit the same emotion scores as the sentence from which they were generated.\n","\n","This ensures that all samples, whether real or synthetic, are labeled consistently and meaningfully for training.\n"],"metadata":{"id":"LGI6IxU5Wu_m"}},{"cell_type":"code","source":["def augmented_dict_to_df(\n","    augmented_wd: Dict[Any, List[Tuple[List[str], int]]],\n","    df_merged: pd.DataFrame,\n","    emotions: List[str]\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Build a DataFrame with columns:\n","       - 'sentence' : the augmented sentence (joined tokens)\n","       - one column per emotion in `emotions`, filled from df_merged\n","\n","    Params\n","    ------\n","    augmented_wd : Dict[class_value, List[(tokens, df_idx)]]\n","      Your word_dict for a single emotion, where each sample is (tokens, original_row_index).\n","    df_merged    : pd.DataFrame\n","      The merged DataFrame that has your emotionâ€columns and whose index is the original rowâ€indices.\n","    emotions     : List of column names in df_merged you want to pull (e.g. ['arousal','valence','interest']).\n","\n","    Returns\n","    -------\n","    pd.DataFrame with columns ['sentence'] + emotions.\n","    \"\"\"\n","    rows = []\n","    for cls, samples in augmented_wd.items():\n","        for tokens, df_idx in samples:\n","            sentence = \" \".join(tokens)\n","            # grab the original scores for all requested emotions\n","            scores = df_merged.loc[df_idx, emotions].to_dict()\n","            rows.append({\"sentence\": sentence, **scores})\n","\n","    return pd.DataFrame(rows)\n","\n","\n","# --------------------\n","# Example usage:\n","\n","# say you only care about augmenting arousal:\n","wd_arousal = augmented_word_dicts[\"arousal\"]\n","# and your merged DF has columns ['label','arousal','valence','interest']\n","df_aug = augmented_dict_to_df(\n","    wd_arousal,\n","    merged,\n","    emotions=[\"arousal\",\"sadness\",\"fear\",\"joy\",\"disgust\",\"anger\",\"surprise\"]\n",")\n","\n"],"metadata":{"id":"QRt9ERX-Yhx4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## export\n","Export the original and augmented sentences with their emotions scores to a csv file for future analysis."],"metadata":{"id":"8e4mEVIQs_av"}},{"cell_type":"code","source":["def export_word_dicts(word_dicts: dict, out_dir: str = \"./exports\"):\n","    \"\"\"\n","    Save each emotionâ€™s word-dictionary to <out_dir>/<emotion>.csv\n","    with columns: class, word\n","    \"\"\"\n","    os.makedirs(out_dir, exist_ok=True)\n","\n","    for emotion, wdict in word_dicts.items():\n","        rows = []\n","        for cls, sentences in wdict.items():\n","            for sent in sentences:\n","                sentence_str = \" \".join(sent)\n","                rows.append({\"class\": cls, \"sentence\": sentence_str})\n","\n","        df = pd.DataFrame(rows)\n","        path = os.path.join(out_dir, f\"{emotion}.csv\")\n","        df.to_csv(path, index=False, encoding=\"utf-8\")\n","\n","export_word_dicts(augmented_word_dicts,\n","                  out_dir=\"/content/drive/MyDrive/emotions_csv\")"],"metadata":{"id":"0rcdggude7xd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Re-Balance + Proof about the generalization effect"],"metadata":{"id":"xlpU7EJ_ced-"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","def cap_augmented_in_3to5_rectangle(df, cap_per_class=2000, seed=42):\n","    \"\"\"\n","    Downsample ONLY augmented rows within the rectangle:\n","      is_augmented == True AND valence in [3,5] AND arousal in [3,5]\n","    so that final counts per class (valence=3/4/5 and arousal=3/4/5) are <= cap_per_class.\n","    Two passes: (1) cap by valence, (2) cap by arousal.\n","    \"\"\"\n","    rng = np.random.RandomState(seed)\n","    df = df.copy()\n","\n","    # Ensure numeric (and drop NAs in these keys if exist)\n","    df['valence'] = pd.to_numeric(df['valence'], errors='coerce')\n","    df['arousal'] = pd.to_numeric(df['arousal'], errors='coerce')\n","    df = df.dropna(subset=['valence', 'arousal'])\n","    df['valence'] = df['valence'].astype(int)\n","    df['arousal'] = df['arousal'].astype(int)\n","\n","    def rect_mask(d):\n","        return (\n","            d['is_augmented'].astype(bool) &\n","            d['valence'].between(2, 5) &\n","            d['arousal'].between(3, 6)\n","        )\n","\n","    # ---- Pass A: cap by VALENCE for classes 3,4,5 ----\n","    for v in (2, 3, 4, 5):\n","        total_v = (df['valence'] == v).sum()\n","        excess = max(0, total_v - cap_per_class)\n","        if excess > 0:\n","            candidates = df.index[rect_mask(df) & (df['valence'] == v)]\n","            if len(candidates) > 0:\n","                n_remove = min(excess, len(candidates))\n","                drop_idx = rng.choice(candidates, size=n_remove, replace=False)\n","                df = df.drop(drop_idx)\n","\n","    # ---- Pass B: cap by AROUSAL for classes 3,4,5 ----\n","    for a in (3, 4, 5, 6):\n","        total_a = (df['arousal'] == a).sum()\n","        excess = max(0, total_a - cap_per_class)\n","        if excess > 0:\n","            candidates = df.index[rect_mask(df) & (df['arousal'] == a)]\n","            if len(candidates) > 0:\n","                n_remove = min(excess, len(candidates))\n","                drop_idx = rng.choice(candidates, size=n_remove, replace=False)\n","                df = df.drop(drop_idx)\n","\n","    return df\n","\n","# === usage ===\n","df_capped = cap_augmented_in_3to5_rectangle(df_balanced, cap_per_class=2000, seed=42)\n","levels = np.arange(1, 8)\n","\n","bef_val = (df_balanced.loc[df_balanced['is_augmented'] == False, 'valence']\n","           .astype(int).value_counts().reindex(levels, fill_value=0))\n","bef_aro = (df_balanced.loc[df_balanced['is_augmented'] == False, 'arousal']\n","           .astype(int).value_counts().reindex(levels, fill_value=0))\n","\n","aft_val = df_capped['valence'].astype(int).value_counts().reindex(levels, fill_value=0)\n","aft_aro = df_capped['arousal'].astype(int).value_counts().reindex(levels, fill_value=0)\n","\n","aug_val = np.maximum(aft_val - bef_val, 0)\n","aug_aro = np.maximum(aft_aro - bef_aro, 0)\n","fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n","\n","# --- Arousal subplot ---\n","axes[0].bar(levels, bef_aro, color=\"#88c999\", alpha=0.7, label=\"Before (no_aug)\")\n","axes[0].bar(levels, aug_aro, bottom=bef_aro, color=\"#a6cee3\", alpha=0.99, label=\"After (aug)\")\n","axes[0].set_title(\"Arousal Distribution\")\n","axes[0].set_xlabel(\"Emotion Level\")\n","axes[0].set_ylabel(\"Count\")\n","axes[0].legend()\n","\n","# --- Valence subplot ---\n","axes[1].bar(levels, bef_val, color=\"#88c999\", alpha=0.7, label=\"Before (no_aug)\")\n","axes[1].bar(levels, aug_val, bottom=bef_val, color=\"#a6cee3\", alpha=0.99, label=\"After (aug)\")\n","axes[1].set_title(\"Valence Distribution\")\n","axes[1].set_xlabel(\"Emotion Level\")\n","axes[1].legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"8VFSscBUcfvv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["balance_count = df_balanced[df_balanced['is_augmented']==False]['sub'].value_counts()\n","capped_count = df_capped['sub'].value_counts()\n","#create one df with both value counts\n","df_count = pd.DataFrame({'original_count': balance_count, 'after_augmentation_count': capped_count})\n","df_count"],"metadata":{"id":"meZr5LI4dGYA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["after_augmentation = df_balanced['sub'].value_counts()\n","original_count = df_balanced[df_balanced['is_augmented']==False]['sub'].value_counts()\n","capped_count = df_capped['sub'].value_counts()\n","\n","df_count = pd.DataFrame({'original_count': original_count,\n","                         'after_augmentation_count': after_augmentation,\n","                         'after_reblance':capped_count})\n","df_count"],"metadata":{"id":"o3VywtVwdSLR"},"execution_count":null,"outputs":[]}]}